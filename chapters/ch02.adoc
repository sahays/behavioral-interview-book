== The Interviewer Perspective
:icons: font
:source-highlighter: highlight.js

[quote, Sherlock Holmes]
____
You see, but you do not observe.
____

The conference room door closes behind the interviewer as she settles into her chair, opens her laptop, and reviews your resume one last time before you arrive. What exactly is going through her mind? What is she looking for? How will she evaluate your responses? And most importantly—what can you do to ensure she sees your true potential?

Understanding the interviewer's perspective is perhaps the most powerful advantage you can have in behavioral interviews. While most candidates focus exclusively on crafting their own responses, those who understand how interviewers think can tailor their answers to address the actual evaluation criteria being used.

In this chapter, we'll pull back the curtain on the interviewer's process. We'll examine the rubrics used by top tech companies, decode their scoring systems, and provide a clear framework for understanding what makes a response exceptional versus merely adequate. By the end, you'll be able to see your answers through the interviewer's eyes—a perspective that will transform how you prepare and perform.

=== Inside the Interviewer's Mind

Before diving into formal rubrics and scoring systems, let's understand the human reality of conducting interviews. Interviewers at tech companies aren't professional interviewers—they're engineers, managers, and leaders who conduct interviews as a small part of their job. This reality shapes how they approach the interview process.

==== The Interviewer's Challenges

Interviewers face several challenges that influence how they evaluate candidates:

* *Cognitive Load*: Most interviewers conduct interviews while balancing their regular workload. They're thinking about the code review due after your interview, the production issue that emerged overnight, and the design document they need to finish by end of day.

* *Limited Training*: While companies like Amazon and Google provide interviewer training, it's typically brief—a few hours at most. Interviewers are expected to learn primarily through experience, which creates inconsistency.

* *Note-Taking Pressure*: Interviewers must capture detailed notes while simultaneously listening to your responses, formulating follow-up questions, and evaluating your answers against rubrics.

* *Recency Effects*: Interviewers typically conduct multiple interviews in succession, creating recency bias where the most recent candidates are remembered more clearly than earlier ones.

* *Comparison Challenges*: Interviewers must compare candidates who have different experiences, different communication styles, and different examples—an inherently difficult task.

These challenges don't mean interviewers are ineffective—just that they're human. Understanding these realities helps you structure your responses in ways that make the interviewer's job easier, which indirectly improves your evaluation.

==== What Interviewers Are Actually Looking For

Beyond specific competencies (which we'll explore in detail), interviewers are fundamentally trying to answer three questions:

1. *Can you do the job?* (Skills and capabilities)
2. *Will you do the job?* (Motivation and alignment)
3. *Will you fit the team and culture?* (Collaboration and values)

Behavioral questions are particularly effective at addressing the second and third questions, which technical interviews often miss entirely. Your examples need to demonstrate not just that you can perform technically, but that you're motivated by the right things and that you work in ways that align with the company's values.

==== The Interviewer's Process

A typical behavioral interview follows this general flow:

1. *Brief introduction and rapport building* (1-2 minutes)
2. *Explanation of the interview format* (1 minute)
3. *First behavioral question* (10-15 minutes including follow-ups)
4. *Second behavioral question* (10-15 minutes including follow-ups)
5. *Possibly a third behavioral question* (if time permits)
6. *Candidate questions* (3-5 minutes)
7. *Wrap-up* (1 minute)

Immediately after you leave, the interviewer will:

1. Complete their notes while memories are fresh
2. Evaluate your responses against specific competencies
3. Assign scores based on company rubrics
4. Submit their evaluation to be considered in hiring discussions

This tight timeline means interviewers are making rapid judgments based on limited information. Your job is to make those judgments as accurate and favorable as possible by providing clear, concrete evidence of your capabilities.

=== What Interviewers Want: Rubrics from Top Tech Companies

While each company has its own evaluation framework, there are remarkable similarities in what top tech companies look for in behavioral interviews. Let's examine the specific rubrics used at Amazon, Google, and Meta to understand what interviewers are evaluating.

==== Amazon: Leadership Principles Evaluation

Amazon's behavioral interview process is explicitly structured around their Leadership Principles. Each interviewer is typically assigned 2-3 principles to assess, with questions designed to elicit evidence of behaviors aligned with those principles.

===== Amazon's Evaluation Rubric

For each Leadership Principle, interviewers evaluate candidates on a four-point scale:

1. *Strong Negative*: Demonstrated behaviors contrary to the principle
2. *Negative*: Insufficient evidence of behaviors aligned with the principle
3. *Positive*: Clear evidence of behaviors aligned with the principle
4. *Strong Positive*: Exceptional evidence of behaviors aligned with the principle, often exceeding expectations

To achieve a "Strong Positive" rating, candidates typically need to demonstrate:

* *Depth*: Detailed, specific examples with clear personal contribution
* *Impact*: Measurable results that mattered to the business
* *Complexity*: Navigation of challenging circumstances or constraints
* *Leadership*: Influence beyond direct responsibilities
* *Reflection*: Thoughtful lessons learned and applied

===== Example: Customer Obsession Evaluation

For Amazon's first Leadership Principle, "Customer Obsession," interviewers look for specific behaviors:

[cols="1,3", options="header"]
|====
|Rating|Evidence Required
|Strong Positive|Candidate provided multiple examples of identifying and addressing customer needs before customers themselves recognized them. Demonstrated willingness to make significant personal or team sacrifices to improve customer experience. Showed measurable impact on customer satisfaction or business metrics.
|Positive|Candidate provided clear examples of focusing on customer needs and working backward from customer problems. Demonstrated understanding of both explicit and implicit customer requirements. Showed how customer focus influenced decisions.
|Negative|Candidate provided only vague references to customers without specific examples of customer-focused behaviors. Examples focused more on technical solutions than customer problems. Limited evidence of considering customer perspective in decision-making.
|Strong Negative|Candidate's examples showed prioritization of other factors (technical elegance, personal convenience, internal politics) over customer needs. Demonstrated resistance to customer feedback or dismissive attitude toward customer concerns.
|====

This detailed rubric explains why generic or technical-focused answers often receive poor evaluations at Amazon, even when the candidate has strong technical skills. Interviewers are explicitly looking for evidence of customer-focused behaviors, not just technical competence.

==== Google: Competency-Based Assessment

Google's behavioral interview process evaluates candidates across four key areas:

1. *General Cognitive Ability*: How you solve problems and learn
2. *Leadership*: How you influence and collaborate
3. *Role-Related Knowledge*: How you apply your expertise
4. *Googleyness*: How you align with Google's culture and values

===== Google's Evaluation Rubric

For each competency, Google interviewers evaluate candidates on a four-point scale:

1. *Does Not Meet*: Insufficient evidence of the competency
2. *Meets Some*: Partial evidence of the competency
3. *Meets*: Clear evidence of the competency
4. *Strongly Meets*: Exceptional evidence of the competency

To achieve a "Strongly Meets" rating, candidates typically need to demonstrate:

* *Initiative*: Proactive problem identification and solution development
* *Impact*: Significant, measurable results that created value
* *Innovation*: Novel approaches or insights beyond standard solutions
* *Collaboration*: Effective work across organizational boundaries
* *Learning*: Rapid acquisition and application of new knowledge

====== Example: Leadership Evaluation

For the Leadership competency, Google interviewers look for specific behaviors:

[cols="1,3", options="header"]
|====
|Rating|Evidence Required
|Strongly Meets|Candidate provided examples of influencing outcomes without formal authority. Demonstrated ability to navigate ambiguity and align diverse stakeholders. Showed how they developed others and created leverage through team empowerment. Examples included measurable team or organizational impact.
|Meets|Candidate provided clear examples of team leadership or cross-functional collaboration. Demonstrated ability to navigate disagreements constructively. Showed how they contributed to team success beyond individual responsibilities.
|Meets Some|Candidate provided examples that showed some leadership elements but lacked depth or impact. Limited evidence of influence beyond direct responsibilities. Examples focused more on individual contribution than team outcomes.
|Does Not Meet|Candidate's examples showed primarily individual work with limited collaboration. Demonstrated difficulty navigating disagreements or influencing others. Limited evidence of considering broader team or organizational context.
|====

This rubric explains why technical brilliance alone isn't sufficient at Google. Interviewers are explicitly evaluating leadership behaviors that enable scale and impact beyond individual contribution.

==== Meta: Impact and Scale Focus

Meta's behavioral interview process emphasizes impact, scale, and speed—values central to the company's culture. Their evaluation framework focuses on:

1. *Impact*: The measurable results of your work
2. *Scale*: The scope and reach of your influence
3. *Speed*: Your ability to execute quickly and effectively
4. *Collaboration*: How you work with others to achieve outcomes
5. *Problem-Solving*: How you approach complex challenges

===== Meta's Evaluation Rubric

For each dimension, Meta interviewers evaluate candidates on a five-point scale:

1. *Poor*: Significant concerns or red flags
2. *Fair*: Below expectations for level
3. *Good*: Meets expectations for level
4. *Excellent*: Exceeds expectations for level
5. *Outstanding*: Significantly exceeds expectations for level

To achieve an "Outstanding" rating, candidates typically need to demonstrate:

* *Quantifiable Impact*: Clear metrics showing significant results
* *Broad Influence*: Effect beyond immediate team or project
* *Velocity*: Rapid execution without sacrificing quality
* *Strategic Thinking*: Connection between tactical work and larger goals
* *Ownership*: End-to-end responsibility for outcomes

====== Example: Impact Evaluation

For the Impact dimension, Meta interviewers look for specific behaviors:

[cols="1,3", options="header"]
|====
|Rating|Evidence Required
|Outstanding|Candidate provided examples with exceptional, quantifiable impact significantly beyond expectations for their role. Demonstrated how their work affected core business metrics or enabled major strategic initiatives. Showed how they identified and captured opportunities that others missed.
|Excellent|Candidate provided examples with clear, measurable impact exceeding expectations for their role. Demonstrated connection between their work and important business outcomes. Showed how they overcame significant obstacles to deliver results.
|Good|Candidate provided examples with solid impact appropriate for their role. Demonstrated understanding of how their work contributed to team or product goals. Showed reasonable metrics or outcomes from their efforts.
|Fair|Candidate provided examples with limited or unclear impact. Metrics were vague or modest relative to role expectations. Limited evidence of connecting work to broader business objectives.
|Poor|Candidate could not provide examples with meaningful impact. Focused on activities rather than outcomes. Showed limited understanding of how their work created value.
|====

This rubric explains why Meta interviewers probe deeply for metrics and quantifiable results. The company's culture emphasizes measurable impact, and the interview evaluation reflects this priority.

=== Scoring Systems Explained

Now that we've examined the specific rubrics used by top tech companies, let's understand how these evaluations translate into hiring decisions. The scoring process is more nuanced than many candidates realize.

==== Individual Question Scoring

Each behavioral question is typically scored independently, based on the specific competencies it was designed to assess. A single interview might evaluate 2-4 competencies through different questions.

For example, an Amazon interviewer might ask:

* Question 1: "Tell me about a time when you had to make a decision without all the information you wanted." (Evaluating "Bias for Action" and "Are Right, A Lot")
* Question 2: "Describe a situation where you had to deliver results with minimal resources." (Evaluating "Frugality" and "Deliver Results")

Each question receives its own score based on the relevant Leadership Principles, rather than a single overall interview score.

==== Competency Aggregation

When multiple questions assess the same competency, or when multiple interviewers assess the same competency in different interviews, these scores are aggregated to form a more complete picture.

For example, if three different interviewers all assessed "Leadership" through different questions, their individual assessments would be combined to create an overall Leadership evaluation.

This aggregation is why consistency across interviews is so important. If you provide contradictory examples or demonstrate different behaviors to different interviewers, it creates confusion in the evaluation process.

==== Level Calibration

Scores are calibrated based on the level of the position you're interviewing for. The same response might receive a "Strong Positive" for a junior role but only a "Positive" for a senior role, because expectations increase with seniority.

This level calibration is particularly important for experienced candidates. What impressed interviewers earlier in your career may be considered merely adequate for senior roles, requiring you to provide more sophisticated examples with greater impact and complexity.

==== Hiring Bar Determination

Each company establishes a "hiring bar" that candidates must clear for an offer. This bar typically includes:

* Minimum score requirements for critical competencies
* Overall score thresholds across all competencies
* No significant concerns or "red flags" in any area

The specific thresholds vary by company and role, but the general principle is consistent: candidates must demonstrate sufficient evidence across all required competencies, with no major gaps or concerns.

==== Leveling Decisions

For candidates who clear the hiring bar, companies then make leveling decisions—determining which job level (and corresponding compensation) to offer. These decisions consider:

* The strength of behavioral interview performance
* The complexity and impact of examples provided
* The scope of leadership and influence demonstrated
* Comparison to current employees at different levels

This leveling process explains why behavioral interview performance can significantly impact compensation, even for candidates who receive offers. The difference between "meeting expectations" and "exceeding expectations" can translate to tens of thousands of dollars in annual compensation.

=== The Behavioral Interview Decision Flowchart

To visualize how interviewers evaluate candidates, let's examine a decision flowchart that represents the typical assessment process:

[.image-center]
image::images/ch02-waterfall.png[candidate response decision tree]

This flowchart illustrates the sequential evaluation process that interviewers follow, consciously or unconsciously, when assessing behavioral responses. Each decision point represents an opportunity for your response to either strengthen or weaken your evaluation.

Let's examine each decision point in detail:

==== Is the example specific and detailed?

Interviewers first determine whether your response provides a concrete, specific example rather than a general philosophy or approach. Responses that begin with phrases like "I usually..." or "My approach is..." rather than describing a specific situation immediately raise concerns.

*What interviewers are thinking*: "Is this candidate providing a real example, or are they speaking in generalities to avoid revealing their actual behavior?"

==== Is there clear evidence of personal contribution?

Once the example is established as specific, interviewers assess whether you've clearly articulated your personal role and contributions. Responses that focus exclusively on what "we" did without clarifying your specific actions create ambiguity about your actual contribution.

*What interviewers are thinking*: "What did this candidate actually do, versus what their team accomplished? Are they taking credit for others' work, or minimizing their own contribution?"

==== Are the actions aligned with the competency being assessed?

Interviewers then evaluate whether your described behaviors demonstrate the specific competency they're assessing. Even impressive examples can receive negative evaluations if they don't demonstrate the targeted competency.

*What interviewers are thinking*: "Does this example actually show customer obsession/leadership/innovation/etc., or is it demonstrating something else entirely?"

==== Were the results measurable and meaningful?

Next, interviewers assess the impact of your actions. Examples with clear, quantifiable results that mattered to the business receive stronger evaluations than those with vague or minimal outcomes.

*What interviewers are thinking*: "Did this candidate's actions actually make a difference? How significant was the impact relative to their role and resources?"

==== Is there evidence of learning and growth?

Finally, interviewers look for reflection and lessons learned—evidence that you've grown from the experience and would apply those insights in future situations.

*What interviewers are thinking*: "Does this candidate learn and adapt from experience? Do they show self-awareness and continuous improvement?"

This sequential evaluation explains why even technically impressive examples can receive poor evaluations if they're missing key elements like specific details, personal contribution, or measurable results.

=== Common Interviewer Concerns and Red Flags

Beyond the formal evaluation criteria, interviewers watch for specific concerns that might indicate future performance issues. Understanding these red flags helps you avoid triggering unnecessary concerns.

==== Lack of Ownership

Interviewers are wary of candidates who deflect responsibility or blame others for negative outcomes. Phrases like "my manager decided..." or "the team failed to..." without acknowledging personal responsibility raise concerns about ownership.

*What interviewers are thinking*: "Will this person take responsibility when things go wrong, or will they blame others and avoid accountability?"

==== Credit Claiming

Conversely, interviewers are concerned by candidates who claim exclusive credit for team accomplishments. Responses that minimize others' contributions or overstate personal impact raise questions about collaboration and honesty.

*What interviewers are thinking*: "Is this person a team player who recognizes others' contributions, or do they prioritize personal recognition over team success?"

==== Lack of Data

Interviewers are skeptical of accomplishments without metrics or measurable outcomes. Vague claims of "significant improvement" or "positive feedback" without specific numbers raise questions about actual impact.

*What interviewers are thinking*: "Did this person actually measure their impact, or are they making unsubstantiated claims about their effectiveness?"

==== Interpersonal Conflicts

Interviewers pay close attention to how candidates describe conflicts with colleagues or managers. Responses that demonstrate contempt, dismissiveness, or inability to see others' perspectives raise concerns about collaboration.

*What interviewers are thinking*: "How will this person handle disagreements on our team? Will they be respectful and constructive, or create interpersonal friction?"

==== Rigidity

Interviewers watch for signs of inflexibility or resistance to change. Examples that show unwillingness to adapt to new information or changing circumstances raise concerns about agility.

*What interviewers are thinking*: "Will this person adapt to our fast-paced environment, or will they resist change and slow the team down?"

==== Lack of Learning

Interviewers are concerned by candidates who can't articulate what they've learned from experiences, particularly challenging ones. Responses that show no growth or reflection suggest limited development potential.

*What interviewers are thinking*: "Does this person learn and grow from experience, or do they repeat the same patterns without improvement?"

==== Misalignment with Values

Finally, interviewers watch for examples that demonstrate values misaligned with the company's culture. Actions that might be rewarded in one environment can be red flags in another.

*What interviewers are thinking*: "Will this person thrive in our culture, or will their natural behaviors create friction with our values and expectations?"

Understanding these concerns helps you frame your examples in ways that address potential red flags proactively, rather than leaving interviewers with unanswered questions.

=== How Interviewers Evaluate Specific Competencies

Different competencies require different types of evidence. Understanding what interviewers look for when assessing specific capabilities helps you select and structure examples appropriately.

==== Leadership Evaluation

When assessing leadership, interviewers look for:

* *Influence without authority*: How you've achieved outcomes through persuasion rather than positional power
* *Vision setting*: How you've created clarity about goals and direction
* *Team development*: How you've helped others grow and improve
* *Decision quality*: How you've made sound judgments with appropriate input
* *Conflict resolution*: How you've navigated disagreements constructively

*Example question*: "Tell me about a time when you had to lead a team through a difficult situation."

*What makes a strong response*: Specific example of aligning a diverse team around a challenging goal, with clear description of how you built consensus, addressed concerns, and achieved measurable results. Include reflection on what you learned about effective leadership.

==== Problem-Solving Evaluation

When assessing problem-solving, interviewers look for:

* *Problem identification*: How you recognize issues before they become critical
* *Root cause analysis*: How you dig beyond symptoms to underlying causes
* *Solution development*: How you generate and evaluate options
* *Implementation*: How you execute solutions effectively
* *Validation*: How you verify that solutions actually solved the problem

*Example question*: "Describe a complex problem you solved. What was your approach?"

*What makes a strong response*: Specific example of tackling a non-trivial problem with clear description of your analytical process, the alternatives you considered, why you selected your approach, and how you measured success. Include reflection on what you'd do differently with hindsight.

==== Innovation Evaluation

When assessing innovation, interviewers look for:

* *Curiosity*: How you explore new ideas and possibilities
* *Risk-taking*: How you pursue opportunities despite uncertainty
* *Experimentation*: How you test hypotheses and learn from results
* *Persistence*: How you overcome obstacles to implementation
* *Impact*: How your innovations created meaningful value

*Example question*: "Tell me about a time when you created a new approach or solution."

*What makes a strong response*: Specific example of identifying an opportunity for innovation, developing a novel approach, navigating implementation challenges, and achieving measurable impact. Include reflection on what you learned about effective innovation.

==== Customer Focus Evaluation

When assessing customer focus, interviewers look for:

* *Customer understanding*: How you identify explicit and implicit needs
* *Empathy*: How you see situations from the customer's perspective
* *Prioritization*: How you make decisions based on customer impact
* *Advocacy*: How you represent customer interests in internal discussions
* *Satisfaction*: How you measure and improve customer experience

*Example question*: "Describe a situation where you had to balance customer needs with other priorities."

*What makes a strong response*: Specific example of deeply understanding customer requirements, advocating for customer interests despite competing priorities, making thoughtful trade-offs, and measuring impact on customer satisfaction. Include reflection on what you learned about effective customer focus.

==== Collaboration Evaluation

When assessing collaboration, interviewers look for:

* *Relationship building*: How you establish effective working relationships
* *Communication*: How you share information and align understanding
* *Conflict resolution*: How you address disagreements constructively
* *Contribution*: How you add value to collective efforts
* *Recognition*: How you acknowledge others' contributions

*Example question*: "Tell me about a time when you had to work with a difficult team member."

*What makes a strong response*: Specific example of navigating a challenging collaboration, with clear description of how you built understanding, found common ground, addressed conflicts directly, and achieved shared goals despite difficulties. Include reflection on what you learned about effective collaboration.

Understanding these specific evaluation criteria helps you select examples that demonstrate the particular competencies being assessed, rather than sharing impressive stories that miss the mark.

=== The Follow-Up Question Strategy

One aspect of behavioral interviews that catches many candidates by surprise is the follow-up question. After you provide your initial response, interviewers typically ask 3-5 follow-up questions to probe more deeply into your example.

These follow-ups aren't random—they're strategic attempts to gather specific information that wasn't provided in your initial response. Understanding the patterns in follow-up questions helps you prepare more effectively and respond more confidently.

==== Types of Follow-Up Questions

Interviewers typically use several types of follow-up questions:

===== Clarification Questions

These questions seek to clarify aspects of your example that were unclear or incomplete:

* "Can you explain your specific role in that project?"
* "What was the timeline for this situation?"
* "Who else was involved in this decision?"

*What interviewers are thinking*: "I need more context to understand what actually happened and what this candidate's contribution was."

===== Depth Questions

These questions probe more deeply into specific aspects of your example:

* "How did you determine that was the right approach?"
* "What alternatives did you consider?"
* "What specific metrics did you use to measure success?"

*What interviewers are thinking*: "I want to understand this candidate's thought process and analytical rigor, not just what they did."

====== Challenge Questions

These questions test your response to pushback or alternative perspectives:

* "What if your approach hadn't worked?"
* "Why didn't you consider [alternative approach]?"
* "How did you respond when people disagreed with your direction?"

*What interviewers are thinking*: "I want to see how this candidate handles challenges and whether they can think flexibly."

====== Reflection Questions

These questions assess your self-awareness and learning orientation:

* "What would you do differently if you faced this situation again?"
* "What did you learn from this experience?"
* "How have you applied those lessons in subsequent situations?"

*What interviewers are thinking*: "I want to understand whether this candidate learns and grows from experience or repeats the same patterns."

====== Hypothetical Extensions

These questions extend your example into hypothetical territory:

* "How would your approach have changed if you had half the time?"
* "What would you have done if your manager had opposed your solution?"
* "How would you adapt this approach for a much larger team?"

*What interviewers are thinking*: "I want to test this candidate's adaptability and whether they can apply their experience to new contexts."

==== Why Follow-Ups Matter

Follow-up questions serve several important purposes in the evaluation process:

* They reveal the depth of your experience and understanding
* They test whether your example is genuine or rehearsed
* They provide additional evidence for competency assessment
* They show how you think on your feet and respond to unexpected questions
* They allow interviewers to address specific concerns raised by your initial response

Your performance on follow-up questions often matters more than your initial response, because it provides more authentic evidence of your capabilities and thought processes.

==== Preparing for Follow-Ups

While you can't predict every follow-up question, you can prepare for the most common patterns:

* For each example in your preparation, identify aspects that might need clarification
* Consider what alternatives you might have pursued and why you didn't
* Reflect deeply on what you learned and how you've applied those lessons
* Think about how you'd adapt your approach to different constraints or contexts
* Prepare to discuss metrics and results in greater detail than your initial response

This preparation ensures you can handle follow-ups confidently, without appearing caught off guard or inconsistent with your initial response.

=== Interviewer Evaluation Techniques

Beyond asking questions, interviewers use specific techniques to evaluate your responses more effectively. Understanding these techniques helps you recognize what's happening during the interview and respond appropriately.

==== The Silence Technique

Interviewers often use strategic silence after you finish speaking, creating an uncomfortable pause that many candidates rush to fill with additional information. This technique reveals:

* Whether you're confident in your initial response or feel the need to keep talking
* What additional information you provide when not specifically prompted
* How you handle momentary social discomfort

*How to respond*: Be comfortable with silence. If you've provided a complete response, simply wait for the next question. Don't undermine a strong answer by rambling to fill silence.

==== The Skepticism Technique

Interviewers sometimes express subtle skepticism about your example, through facial expressions, tone, or gentle pushback. This technique reveals:

* How you respond to implicit challenges
* Whether you become defensive or remain composed
* If you can provide additional evidence to support your claims

*How to respond*: Address perceived skepticism with additional specific details and metrics, not by becoming defensive or overemphasizing your claims. Provide evidence rather than assertions.

==== The Constraint Technique

Interviewers may impose artificial constraints on your responses, such as "Tell me about that in two minutes or less" or "Focus just on your specific actions." This technique reveals:

* Your ability to communicate concisely
* How you prioritize information under constraints
* Whether you can adapt your communication to specific requirements

*How to respond*: Respect the constraint and adapt your response accordingly. Demonstrate that you can adjust your communication style based on the interviewer's needs.

==== The Redirection Technique

Interviewers sometimes redirect you in the middle of a response, asking about a different aspect of your example than what you were discussing. This technique reveals:

* Your flexibility and adaptability
* How you handle interruptions
* Whether you can shift focus while maintaining coherence

*How to respond*: Acknowledge the redirection and shift your focus accordingly, without showing frustration or rigidly returning to your previous point. Demonstrate that you can follow the interviewer's lead.

==== The Probing Technique

Interviewers often ask increasingly specific questions about your example, drilling down into details that might seem minor. This technique reveals:

* Whether your example is genuine and lived
* The depth of your involvement and understanding
* How thoroughly you've reflected on the experience

*How to respond*: Provide specific, detailed answers to probing questions, even if they seem tangential. The specificity of your responses demonstrates the authenticity of your example.

Understanding these techniques helps you recognize what's happening during the interview and respond in ways that strengthen rather than weaken your evaluation.

=== Conclusion: The Strategic Advantage of the Interviewer's Perspective

Understanding the interviewer's perspective transforms behavioral interviews from mysterious evaluations into strategic opportunities. When you know what interviewers are looking for, how they evaluate responses, and what concerns they're trying to address, you can craft examples that directly address their actual assessment criteria.

This perspective shift offers several advantages:

* You can select examples that demonstrate the specific competencies being evaluated, rather than sharing impressive stories that miss the mark
* You can structure responses to include all the elements interviewers are looking for, rather than leaving critical gaps
* You can anticipate and prepare for follow-up questions, rather than being caught off guard
* You can recognize and respond appropriately to interviewer techniques, rather than misinterpreting their intentions
* You can avoid common red flags that trigger unnecessary concerns, even when your actual capabilities are strong

In the next chapter, we'll build on this understanding by exploring the STAR+ framework—a powerful structure for organizing your responses in ways that address the evaluation criteria we've examined. This framework will help you translate your understanding of the interviewer's perspective into practical, effective interview responses that showcase your true capabilities.
